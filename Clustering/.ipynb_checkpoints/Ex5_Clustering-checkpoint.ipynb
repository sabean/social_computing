{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Computing - Summer 2019\n",
    "# Exercise 5 - Clustering\n",
    "Clustering is analyzing data structures (regarding this exercise, especially datasets and graphs) according to certain characteristics of interest and grouping the points whose attributes overlap. This procedure yields new disjoint groupings, the clusters the points belong to, which may reveal insightful information. In the following, we will take a look at two different clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5.1: k-Means Clustering\n",
    "\n",
    "In the first section of the exercise, you will work with the dataset *NetworkActivityData.csv* which documents the user's activity of an online social network. Each line in the file represents a feature vector and is associated with one of the users. The vectors contain an ID ('userXYZ') and the following four different activity types (features) of that user:\n",
    "* Number of posts\n",
    "* Number of comments\n",
    "* Number of likes (on posts and comments)\n",
    "* Number of friends\n",
    "\n",
    "By clustering the users according to all of the features, you can identify similar activity patterns among them. In practice, this can be helpful for research but also for advertising and polling firms. Your first task will therefore be to implement the k-means clustering you are already familiar with from the lecture.\n",
    "\n",
    "### k-Means Algorithm\n",
    "The algorithm's goal is to partition the data points (also referred to as _observations_) into k sets while minimizing the within-cluster sum of squares. The goal of the k-means clustering is to minimize the objective function\n",
    "$$\\sum_{k=1}^{K} \\sum_{\\{n|x_n \\in C_k \\}} \\|x_n - \\mu_k\\|^2$$\n",
    "\n",
    "such that $K$ is the number of clusters, $x_n$ is the $n$-th point that belongs to the $k$-th cluster, and $\\mu_k$ is the centroid (or _prototype_) of the $k$-th cluster.\n",
    "    \n",
    "The algorithm's implementation proceeds as follows:\n",
    "* Assign four random centroids (starting prototypes)\n",
    "* Assign data points (the users) to the nearest centroid\n",
    "* Recompute the centroid values - the new value of the $k$-th centroid is the average of the points currently assigned to that centroid\n",
    "* Repeat from point 3 until the centroids' values do not change anymore\n",
    "\n",
    "**Write a Python program that computes the k-means clustering for the given dataset with a k value of 4 by following the tasks below.** The output of your program should be a dictionary (or whatever data type works best for you) that assigns a cluster ID (0, 1, 2, 3) to every user in the input file. The first argument in that tuple should be the users's name and the second argument should be the centroid ID to which this user is associated to, e.g. ('user111', 3).<br>\n",
    "After the clustering with k=4 is implemented, run the code with the following starting prototypes for testing your solution: **{0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}**\n",
    "\n",
    "**Notes:**\n",
    "* The return value should be the final centroid values that do not change anymore.\n",
    "* If you happen to come across any empty clusters in your implementation (e.g. if a clusters did not get assigned any data points), they need to be reinitialized in order to return all k clusters later. A common way to do this is using one or more random points far away from their centroid. In our case, it is sufficient to set an empty cluster to all zeros (a value that is located at the data's boundaries), which creates a similar effect. _For the future, keep in mind that this is not always a good idea because it depends on the specific underlying space!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, random, queue, numpy as np, igraph as ig\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Reading The Data\n",
    "\n",
    "The first thing you are supposed to do is reading in the data set (_NetworkActivityData.csv_). As mentioned above, the file's rows contain a social network user with their activity types (posts, comments, likes and friends). One possibility is to save the data as a dictionary, the user ID as a key and the list of activity types as its values.\n",
    "\n",
    "**Implement a function that reads in a .csv file given a certain path so you can call it later on the data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network activity data set into a dictionary with user IDs as keys\n",
    "def read_data_set(path):\n",
    "    observations_dict = {}\n",
    "    \n",
    "    # TODO: Open .csv file\n",
    "    \n",
    "        # TODO: Put each row (data for each user) into dictionary of observations\n",
    "        \n",
    "    \n",
    "    return observations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Starting Centroids\n",
    "In order to use k-means clustering, we need k starting centroids to begin with. There are different ways to do it, one of them is randomly assigning the initial values. For each dimension of the centroid, the random values should fall between the dimension's maximum and minimum value of all the points in the data set.<br>\n",
    "For example in our case, if the dimension \"number of comments\" has a maximum value of 49 and a minimum value of 0, the value assigned to the third component of one centroid should be randomly assigned between 0 and 49. The same for the other features and centroids, of course.\n",
    "\n",
    "**Implement a function that computes k random starting centroids, bounded within the minimum and maximum dimension of the input features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute k random starting centroids\n",
    "def create_starting_centroids(observations, k):\n",
    "    # TODO: Initialize minimum/maximum with arbitrary observation entry\n",
    "    mins = \n",
    "    maxs = \n",
    "    \n",
    "    # TODO: Find minimum and maximum of observations in each dimension\n",
    "    \n",
    "\n",
    "    # TODO: Create dictionary with four random centroids within the observed space\n",
    "    centroids = {}\n",
    "    \n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Updating The Clusters\n",
    "The core functionality of the k-means algorithm is to assign each observation (user and their properties) to its new cluster and afterwards to recompute the centroid average. These two steps are repeated until the centroids do not change anymore, meaning that the clusters are finally set.\n",
    "\n",
    "Implement two functions, one that **updates the cluster affiliation for each observation by determining the closest centroid** and **another to update the centroid values by averaging their data points**, but only for one iteration in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each of the data points to its closest centroid\n",
    "def update_observation_assignments(observations, centroids, k):\n",
    "    observation_centroids = {}\n",
    "    \n",
    "    # Create dictionary mapping each observation to a centroid index\n",
    "    for key in observations:\n",
    "        \n",
    "        # TODO: Get distance to all centroids for current observation\n",
    "        \n",
    "        # TODO: Assign centroid that has minimum L2/Euclidean distance\n",
    "        \n",
    "\n",
    "    return observation_centroids\n",
    "\n",
    "\n",
    "# Create new centroid values for each cluster as the mean of its data points\n",
    "def update_centroid_values(observations, observation_centroids, k):    \n",
    "    obs_count = [0] * 4\n",
    "    centroids = {}\n",
    "    \n",
    "    # TODO: Initialize centroids\n",
    "    \n",
    "    # TODO: Accumulate observations in each cluster\n",
    "\n",
    "    # TODO: Get cluster mean\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: k-Means Loop\n",
    "As a last step, k-means uses the functionality that was implemented in the tasks beforehand in a loop and terminates after a few iterations when the centroid values do not change anymore.\n",
    "\n",
    "**Implement k-means clustering by iteratively using the functions defined above until the centroids are set. Afterwards, run your implementation with k=4 using the given starting centroids {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]} instead of the random initialization.**\n",
    "\n",
    "**Hint:** You can verfiy your solution's correctness by comparing your final cluster observations to the pre-defined function's output:<br>\n",
    "`from sklearn.cluster import KMeans`<br>\n",
    "`from numpy import genfromtxt`<br>\n",
    "<br>\n",
    "`observations = genfromtxt('NetworkActivityData.csv', delimiter=',')[:, 1 : 5]`<br>\n",
    "`centers = np.asarray([[9, 33, 29, 25], [4, 44, 12, 41], [10, 13, 44, 65], [10, 44, 48, 70]])`<br>\n",
    "`kmeans = KMeans(n_clusters=4, init=centers).fit(observations)`<br>\n",
    "`result = kmeans.predict(observations)`<br>\n",
    "`print(sum(result == 0), sum(result == 1), sum(result == 2), sum(result == 3))`\n",
    "    \n",
    "**Output the observation counts in each cluster. Finally, have a look at the results and describe the common properties of the four groups.** What information do we have on activity patterns for different groups and for different features? How much did they contribute in the social network? Don't write more than 5 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of k-means clustering\n",
    "def calculate_k_means_clustering(data_set_path, k):\n",
    "    observations = read_data_set(data_set_path)\n",
    "    \n",
    "    centroids = create_starting_centroids(observations, k)\n",
    "    #centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}\n",
    "    \n",
    "    observation_centroids = update_observation_assignments(observations, centroids, k)\n",
    "    new_centroids = update_centroid_values(observations, observation_centroids, k)\n",
    "\n",
    "    # TODO: Loop until centroids do not change anymore\n",
    "    while\n",
    "        centroids = new_centroids\n",
    "        \n",
    "        # TODO: Update cluster assignments and centroid values by calling the functions\n",
    "        \n",
    "    \n",
    "    print('Final assignments:', observation_centroids)\n",
    "    \n",
    "    observation_count = [0] * 4\n",
    "    # TODO: Count observations per cluster and output results\n",
    "    \n",
    "    for i in range(0, k):\n",
    "        print('Average of cluster ', i, ':', new_centroids[i])\n",
    "\n",
    "\n",
    "# Run k-means clustering\n",
    "calculate_k_means_clustering('NetworkActivityData.csv', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5.2: Girvan-Newman Clustering \n",
    "In this section of the exercise, you will look at another clustering method from the lecture, the Girvan-Newman algorithm. It is an efficient algorithm for computing graph clustering. Your task will be to implement it.\n",
    "\n",
    "### Girvan-Newman Algorithm\n",
    "The central goal of the algorithm is to remove edges according to edge betweenness until the best clustering is obtained, based on modularity as an optimality criterion. You already know that a large betweenness value (for a node) indicates that the corresponding edge is a bridge between two communities in the graph. Cutting that edge means isolating these and yielding separate clusters. The modularity is given by the following formula:<br>\n",
    "$$ Q = \\sum_{i} (e_{ii} - a_i^2) $$\n",
    "with $e_{ii}$ being the number of edges that connect nodes within the $i$-th cluster, and $a_i$ being the fraction of edges that connect nodes from outside to nodes of the $i$-th cluster.\n",
    "\n",
    "The algorithm's implementation proceeds as follows:\n",
    "* Calculate the edge betweenness for all edges in the graph\n",
    "* Remove the edge with highest betweenness\n",
    "* Calculate (and update) the modularity for the resulting clustering\n",
    "* Repeat steps 1 to 3 until there are no edges left\n",
    "\n",
    "**Write a Python program that computes the optimal graph clustering for the Krackhardt Kite graph by completing the tasks below.** The program's input is the imported Krackhardt Kite as an igraph Graph object. The output should be a tuple of arguments, the value of the optimal modularity value and the corresponding Graph object representing the best clustering. You do _not_ have to calculate the edge betweenness yourself, you can use igraph's built-in function. Nevertheless, you are free to do it as an optional task (see below).\n",
    "\n",
    "### Task 1: Modularity\n",
    "In order to compare different clusterings later, the first thing to do is **implementing a function for the modularity calculation** by means of the formula given above. The idea is, for each cluster $i$, to calulcate the fraction of edges within the cluster and the fraction of edges that connects to the $i$-th cluster from outside it.\n",
    "\n",
    "**Notes:**\n",
    "* It may be helpful to look at the lecture's slides on the topic again for understanding what the formula stands for.\n",
    "* You are free to implement everything on your own (as long as the result is correct), but a possible way to solve it is by working with dictionaries of the original and current graph's degrees.\n",
    "* It is useful to add an attribute 'name' to the original graph's nodes in order to preserve the indices as they are reset when creating subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary that maps each node to its degree\n",
    "def calculate_node_degrees(g):\n",
    "    deg_dict = {}\n",
    "    \n",
    "    # TODO: # Map calculcate degree centrality\n",
    "        \n",
    "    return deg_dict\n",
    "\n",
    "\n",
    "# Calculates the modularity of graph g using the original and current degree mappings\n",
    "def calculate_modularity(g, orig_deg_dict, num_edges):\n",
    "    Q = 0\n",
    "    components = g.components()\n",
    "    deg_dict = calculate_node_degrees(g)\n",
    "    \n",
    "    # Loop over clusters and count edges\n",
    "    for i in range(len(components)):\n",
    "        subgraph = components.subgraph(i)\n",
    "        e = 0 # Fraction of edges within cluster\n",
    "        a = 0 # Fraction of edges between in-cluster nodes and nodes outside\n",
    "        \n",
    "        for v in subgraph.vs:\n",
    "            e += # TODO: Get the current degree dict entry, pay attention to indices\n",
    "            a += # TODO: Get the original degree dict entry, pay attention to indices\n",
    "            \n",
    "        # TODO: Calculate modularity\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Girvan-Newman Loop\n",
    "The next step is to implement the core functionality. Determine the **betweenness values for all edges** in the current graph first and remove the one with the highest betweenness value to get a meaningful clustering. Additionally, calculate the **graph's modularity value using the function defined above and keep track of the largest one**. This is repeated until no edges are left anymore. The final output is the graph with the best clustering and the corresponding modularity value.\n",
    "\n",
    "In order to test your implementation, compare your results to the built-in functions' solution. The  function `community_edge_betweenness()` returns a dendrogram whose clustering (obtained by using `as_clustering()` on it) you can plot. If you call a graph's `modularity()` function with that clustering, you can check the optimal modularity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Implementation Girvan-Newman clustering\n",
    "def calculate_girvan_newman_clustering(g):\n",
    "    num_edges = # TODO: Get number of edges\n",
    "    largest_Q = 0.0\n",
    "\n",
    "    final_clustering = None\n",
    "    orig_deg = calculate_node_degrees(g)\n",
    "    \n",
    "    # TODO: Preserving original indices as node attributes (for later modularity computation)\n",
    "        \n",
    "    # TODO: Remove all edges according to their modularity until none are left\n",
    "    while \n",
    "        # Compute modularity for remaining edges\n",
    "        Q = calculate_modularity(g, orig_deg, num_edges)\n",
    "\n",
    "        if Q > largest_Q:\n",
    "            # TODO: Update largest modularity value and copy the corresponding graph\n",
    "        \n",
    "        # TODO: Compute betweenness for each edge and get highest index\n",
    "\n",
    "        # TODO: Delete edge with highest index\n",
    "    \n",
    "    return final_clustering, largest_Q\n",
    "\n",
    "\n",
    "# Import the Kite\n",
    "g = ig.Graph.Famous('Krackhardt_Kite')\n",
    "print('Original Graph\\n', g)\n",
    "\n",
    "# Compute clustering and largest modularity value\n",
    "graph_clustering, largest_mod = calculate_girvan_newman_clustering(g)\n",
    "print('\\nLargest modularity value: ', largest_mod)\n",
    "print('\\nClustered Graph\\n', graph_clustering)\n",
    "\n",
    "# Plot the clustered graph\n",
    "ig.plot(graph_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing your implementation\n",
    "g = ig.Graph.Famous('Krackhardt_Kite')\n",
    "\n",
    "# TODO: Show dendrogram, the optimal modularity and corresponding clustering\n",
    "\n",
    "# TODO: Plot the clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Task 3: Edge Betweenness\n",
    "As an optional task, you can implement the edge betweenness calculation that is given by the `edge_betweenness()` function above. You **do not** have to do this task in order to get the full grade but you can still improve if you were not able to solve the problems 5.1 or 5.2 sufficiently.\n",
    "    \n",
    "**Implement the edge betweenness based on Brandes' algorithm presented in [1].**\n",
    "\n",
    "[1] U. Brandes: _A Faster Algorithm for Betweenness Centrality._ 2001. ([PDF](https://algo.uni-konstanz.de/publications/b-fabc-01.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edge_betweenness(g):\n",
    "    C = {} # Edge betweenness dictionary\n",
    "    \n",
    "    # Calculate edge betweenness according to Brandes algorithm\n",
    "    \n",
    "    return C"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
